import os
import shutil
# --- S·ª¨A L·ªñI: D√πng th∆∞ vi·ªán m·ªõi langchain_text_splitters ---
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

# --- C·∫§U H√åNH ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(CURRENT_DIR)
DATA_ROOT = os.path.join(PROJECT_ROOT, "data_raw")
DB_PATH = os.path.join(PROJECT_ROOT, "vector_db")

# C√°c t·ª´ kh√≥a nh·∫≠n di·ªán
KNOWN_PUBLISHERS = ["CanhDieu", "KetNoiTriThuc", "ChanTroiSangTao"]
KNOWN_TYPES = ["SGK", "SGV", "SBT"]

def get_metadata_from_path(file_path):
    path_parts = file_path.split(os.sep)
    metadata = {
        "grade": "Unknown",
        "publisher": "TongHop",
        "book_type": "TaiLieuKhac"
    }
    # 1. T√¨m L·ªõp
    for part in path_parts:
        if part.startswith("Lop") and part[3:].isdigit():
            metadata["grade"] = part
            break
    # 2. T√¨m NXB
    for pub in KNOWN_PUBLISHERS:
        if pub in path_parts:
            metadata["publisher"] = pub
            break
    # 3. T√¨m Lo·∫°i s√°ch
    for b_type in KNOWN_TYPES:
        if b_type in path_parts:
            metadata["book_type"] = b_type
            break
    return metadata

def ingest_data():
    print("üöÄ B·∫Øt ƒë·∫ßu n·∫°p d·ªØ li·ªáu (Version: pymupdf4llm)...")
    
    if not os.path.exists(DATA_ROOT):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c '{DATA_ROOT}'")
        return

    documents = []
    total_files = 0
    total_pages_with_text = 0

    # Th·ª≠ import pymupdf4llm
    try:
        import pymupdf4llm
        use_llm_extractor = True
        print("‚úÖ S·ª≠ d·ª•ng pymupdf4llm cho text extraction")
    except ImportError:
        use_llm_extractor = False
        print("‚ö†Ô∏è pymupdf4llm kh√¥ng c√≥, s·ª≠ d·ª•ng fitz tr·ª±c ti·∫øp")

    # Qu√©t file b·∫±ng os.walk (b·∫•t ch·∫•p c·∫•u tr√∫c th∆∞ m·ª•c)
    for root, dirs, files in os.walk(DATA_ROOT):
        for filename in files:
            if filename.endswith(".pdf"):
                file_path = os.path.join(root, filename)
                meta = get_metadata_from_path(file_path)
                
                if meta["grade"] != "Unknown":
                    try:
                        print(f"üìñ ƒê·ªçc: {filename} \t| {meta['grade']} - {meta['publisher']} - {meta['book_type']}")
                        
                        if use_llm_extractor:
                            # S·ª≠ d·ª•ng pymupdf4llm ƒë·ªÉ extract markdown (t·ªët h∆°n cho LLM)
                            md_text = pymupdf4llm.to_markdown(file_path)
                            
                            if md_text and md_text.strip():
                                # Chia nh·ªè theo sections
                                sections = md_text.split('\n\n')
                                for i, section in enumerate(sections):
                                    section = section.strip()
                                    if len(section) > 50:  # B·ªè qua sections qu√° ng·∫Øn
                                        doc_obj = Document(
                                            page_content=section,
                                            metadata={
                                                **meta,
                                                'source_file': filename,
                                                'section': i + 1
                                            }
                                        )
                                        documents.append(doc_obj)
                                        total_pages_with_text += 1
                        else:
                            # Fallback: d√πng fitz tr·ª±c ti·∫øp
                            import fitz
                            doc = fitz.open(file_path)
                            
                            for page_num, page in enumerate(doc):
                                text = page.get_text()
                                
                                if text and text.strip():
                                    doc_obj = Document(
                                        page_content=text.replace('\n', ' ').strip(),
                                        metadata={
                                            **meta,
                                            'source_file': filename,
                                            'page': page_num + 1
                                        }
                                    )
                                    documents.append(doc_obj)
                                    total_pages_with_text += 1
                            
                            doc.close()
                        
                        total_files += 1
                        print(f"   ‚Üí Documents so far: {len(documents)}")
                    except Exception as e:
                        print(f"‚ö†Ô∏è L·ªói file {filename}: {e}")

    print(f"\nüìä Th·ªëng k√™:")
    print(f"   - Files ƒë·ªçc: {total_files}")
    print(f"   - Documents t·∫°o: {len(documents)}")

    if len(documents) == 0:
        print("‚ùå Kh√¥ng c√≥ document n√†o ƒë∆∞·ª£c t·∫°o. PDFs c√≥ th·ªÉ l√† d·∫°ng ·∫£nh scan.")
        print("üí° G·ª£i √Ω: C·∫ßn s·ª≠ d·ª•ng OCR ƒë·ªÉ extract text t·ª´ ·∫£nh.")
        return

    print(f"\nüì¶ ƒêang chia nh·ªè {len(documents)} documents...")

    # --- S·ª¨ D·ª§NG CLASS T·ª™ G√ìI M·ªöI ---
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
        separators=["\n\n", "\n", ".", " ", ""]
    )
    chunks = text_splitter.split_documents(documents)
    print(f"üß© S·ªë l∆∞·ª£ng chunks: {len(chunks)}")

    if len(chunks) == 0:
        print("‚ùå Kh√¥ng c√≥ chunks n√†o ƒë∆∞·ª£c t·∫°o.")
        return

    # --- X√ÅC NH·∫¨N: ƒê√É S·ª¨ D·ª§NG MODEL B·∫†N Y√äU C·∫¶U ---
    print("üß† ƒêang t·∫£i Model 'keepitreal/vietnamese-sbert'...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="keepitreal/vietnamese-sbert", # <--- Ch√≠nh x√°c l√† model n√†y
        model_kwargs={'device': 'cpu'}
    )

    if os.path.exists(DB_PATH):
        shutil.rmtree(DB_PATH) # X√≥a DB c≈© ƒë·ªÉ l√†m s·∫°ch
        print("üóëÔ∏è  ƒê√£ x√≥a DB c≈©.")

    print("üíæ ƒêang l∆∞u v√†o ChromaDB...")
    db = Chroma.from_documents(chunks, embedding_model, persist_directory=DB_PATH)
    print(f"üéâ XONG! D·ªØ li·ªáu ƒë√£ l∆∞u t·∫°i: {DB_PATH}")
    print(f"   - T·ªïng chunks: {len(chunks)}")

if __name__ == "__main__":
    ingest_data()